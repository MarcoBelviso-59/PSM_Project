# Experiments (DS3–DS5) — Valutazione sperimentale e confronto baseline (PSM_Project)

Questa cartella contiene (o conterrà) gli strumenti per eseguire la valutazione sperimentale del progetto, in coerenza con i diagrammi DS3–DS5: esecuzione batch su insiemi di password, confronto con una o più baseline, generazione di risultati ripetibili ed export in formati utili (CSV/JSON) per tabelle e grafici. Il principio guida è semplice: gli esperimenti non devono “reinventare” lo scoring, ma invocare sempre l’engine in `src/engine/` (single source of truth), così i risultati sono coerenti con la demo UI e con l’API.

Il contratto minimo con l’engine, coerente con il prototipo, è: l’esperimento invoca `evaluate(password, personalTokens)` per ottenere `{ score, level, patterns }` e (quando necessario) invoca `generateFeedback(evaluation)` per ottenere i suggerimenti; l’accettazione finale può essere verificata con `validateFinal(password, personalTokens)` restituendo `{ ok, msg }`. Anche negli esperimenti, l’input “personale” del progetto resta `personalTokens`, cioè un array di token normalizzati derivati da nome/cognome/email (quando disponibili) e usati per penalizzare password che contengono informazioni personali. Per evitare ambiguità, gli esperimenti devono produrre risultati deterministici: stesso dataset e stesse opzioni → stesso output.

Gli esperimenti lavorano su un dataset che può essere fornito in JSON oppure CSV, con un formato volutamente essenziale. Ogni riga/record rappresenta un caso e deve contenere almeno `password`; può contenere opzionalmente campi utente (`firstName`, `lastName`, `email`) oppure direttamente `personalTokens`. Se `personalTokens` è presente, ha priorità; se non è presente ma sono presenti dati utente, l’esperimento deriva i token seguendo la stessa filosofia del prototipo (normalizzazione, tokenizzazione dell’email, scarto dei token troppo corti). Un esempio concettuale di record JSON è: `{ "id": "case_001", "password": "P@ssw0rdExample!", "personalTokens": ["mario","rossi","example"], "meta": { "source": "synthetic" } }`, mentre in CSV la stessa informazione può essere rappresentata con colonne come `id,password,personalTokens,firstName,lastName,email,source`, dove `personalTokens` è una lista separata da virgole o pipe (es. `mario|rossi|example`). L’obiettivo non è “standardizzare il mondo”, ma avere un formato stabile e facile da costruire per poter ripetere gli esperimenti senza confusione.

L’output degli esperimenti deve essere sempre esportabile in due formati: un JSON “ricco” (adatto al debug e alle analisi dettagliate) e un CSV “tabellare” (adatto a grafici e tabelle). Il JSON ricco deve includere, per ogni caso: identificativo, password (eventualmente oscurata o omessa se necessario), token personali utilizzati (se non sensibili), risultato dell’engine (`score`, `level`, `patterns`), esito di `validateFinal` se calcolato (`ok`, `msg`), e campi di supporto per analisi (ad esempio `length`, categorie di caratteri presenti, numero pattern rilevati). Il CSV tabellare deve includere almeno: `id`, `score`, `level`, `ok`, `length`, indicatori booleani (hasLower/hasUpper/hasDigit/hasSymbol), conteggio pattern e, se si fa confronto, colonne dedicate alla baseline (ad esempio `baseline_name`, `baseline_score`, `delta_score`). La regola è: il CSV deve essere “flat” e pronto per fare grafici/aggregate senza parsing complesso.

Per il confronto con baseline (parte essenziale della valutazione sperimentale), questo modulo deve supportare almeno una baseline selezionata dal team (strumento/metriche di riferimento) e produrre un confronto esplicito nei risultati. La baseline può essere implementata come “adapter” separato che, dato lo stesso input (password + eventuali token), produce un punteggio comparabile o una classe; l’esperimento registra entrambe le valutazioni e calcola differenze (delta) e casi di disaccordo (es. PSM alto vs baseline bassa, o viceversa). La scelta della baseline e delle metriche va documentata in `docs/04_valutazione_sperimentale/README.md`, insieme a motivazioni, minacce alla validità e limiti del dataset; questa cartella `src/experiments/` si occupa invece dell’esecuzione e della produzione di dati grezzi e aggregabili.

Dal punto di vista operativo, l’organizzazione interna consigliata di questa cartella è: un runner che legge dataset e opzioni, invoca engine (e baseline se prevista), e scrive output in una directory di risultati versionabile (ad esempio `src/experiments/outputs/` con sottocartelle per esecuzione come `run_YYYYMMDD_HHMM/`). Ogni run deve produrre: (1) `results.json` ricco, (2) `results.csv` tabellare, (3) un piccolo file di metadati (anche solo JSON) con configurazione dell’esecuzione: data/ora, versione/commit (se disponibile), opzioni attive (penalità, caps, dizionari), nome dataset e numero casi. Questo rende la sperimentazione ripetibile e difendibile in relazione.

Privacy e sicurezza: se decideremo di utilizzare dataset che possono includere password reali o dati personali, dovremo evitare di versionare materiale sensibile. In questo caso, il runner deve supportare una modalità in cui la password non viene salvata nei risultati (o viene salvato solo un identificativo/hash non reversibile), e la repo deve escludere dataset reali da commit (tramite .gitignore e regole condivise). L’obiettivo è poter dimostrare metodologia e risultati senza introdurre rischi.

Criteri pratici per considerare “completato” il modulo esperimenti: esiste un dataset di prova (anche sintetico) con un numero sufficiente di casi (minimo consigliato 100 per avere distribuzioni sensate), il runner produce sempre `results.json` e `results.csv` con colonne/campi stabili, il confronto con almeno una baseline è realizzabile e registrato (anche su subset), e in `docs/04_valutazione_sperimentale/` è possibile creare grafici/tabelle direttamente a partire dagli export senza ulteriori trasformazioni manuali. Collegamenti nel progetto: la UI in `src/web/` serve per la demo e per verifiche manuali; l’API in `src/api/` può essere usata come alternativa di invocazione, ma per performance e controllo gli esperimenti dovrebbero preferire l’invocazione diretta dell’engine; la documentazione dei risultati e delle metriche va raccolta in `docs/04_valutazione_sperimentale/` e poi inserita nella relazione finale in `docs/05_relazione/`.

